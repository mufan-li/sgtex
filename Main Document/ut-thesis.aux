\relax 
\citation{FeHeKh12}
\citation{BaRoYo14}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sc:intro}{{1}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Supervised Learning}{2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:supervised_learning}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Feed-forward Neural Networks}{2}}
\newlabel{sc:nnet}{{2.1}{2}}
\@writefile{toc}{\contentsline {paragraph}{}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  A generalized linear model represented in graphical form. In a neural network, this is also referred to as a single neuron. \relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:GLM}{{2.1}{3}}
\citation{Bi07}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  A generalized feed-forward neural network with two hidden layers. Bias parameters are not drawn for compactness, although they are present in all forward passing nodes. \relax }}{4}}
\newlabel{fig:NN}{{2.2}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Common Techniques to Improve Training}{5}}
\newlabel{sc:tech}{{2.2}{5}}
\@writefile{toc}{\contentsline {paragraph}{}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Mini-Batches}{5}}
\newlabel{subsc:mini_batches}{{2.2.1}{5}}
\@writefile{toc}{\contentsline {paragraph}{}{5}}
\citation{Ne13}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces The Mini-Batch Gradient Descent\relax }}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Momentum and Adam}{6}}
\newlabel{subsc:momemtum}{{2.2.2}{6}}
\@writefile{toc}{\contentsline {paragraph}{}{6}}
\citation{KiBa14}
\citation{SrHi14}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Adam Algorithm\relax }}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Dropout}{7}}
\newlabel{subsc:dropout}{{2.2.3}{7}}
\@writefile{toc}{\contentsline {paragraph}{}{7}}
\citation{Br01}
\citation{GaGh15}
\citation{DaLa12}
\citation{IoSz15}
\citation{Le98}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Batch Normalization}{9}}
\newlabel{subsc:batch_norm}{{2.2.4}{9}}
\@writefile{toc}{\contentsline {paragraph}{}{9}}
\@writefile{loa}{\contentsline {algocf}{\numberline {3}{\ignorespaces The Batch Normalization Algorithm\relax }}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}MNIST Hand-Written Digits Example}{10}}
\newlabel{sc:MNIST}{{2.3}{10}}
\@writefile{toc}{\contentsline {paragraph}{}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces MNIST hand-written digits modeled using a 8 hidden layer neural network, with negative log-likelihood and classification error rate computed after each epoch.\relax }}{11}}
\newlabel{fig:mnist_results}{{2.3}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces MNIST hand-written digits test results with different neural networks.\relax }}{12}}
\newlabel{tab:mnist_results}{{2.1}{12}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Unsupervised Learning}{13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:unsupervised_learning}{{3}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Restricted Boltzmann Machines}{13}}
\newlabel{sc:rbm}{{3.1}{13}}
\@writefile{toc}{\contentsline {paragraph}{}{13}}
\citation{SaMnHi07}
\citation{SaMnHi07}
\citation{SaMnHi07}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces  A restricted Boltzmann machine (RBM) with 4 courses and 5 hidden nodes for a specific student. \relax }}{14}}
\newlabel{fig:RBM}{{3.1}{14}}
\citation{VLLBM10}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Denoising Autoencoders}{15}}
\newlabel{sc:dae}{{3.2}{15}}
\@writefile{toc}{\contentsline {paragraph}{}{15}}
\citation{VLLBM10}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  An one layer autoencoder with 4 input nodes and 3 representation nodes. \relax }}{16}}
\newlabel{fig:AE1}{{3.2}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  An one layer denoising autoencoder with 4 input nodes, 3 representation nodes, and symmetric weights. In this case, we have the third input corrupted by setting to zero. \relax }}{17}}
\newlabel{fig:DAE1}{{3.3}{17}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiment Results}{18}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{ch:results}{{4}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Classification of Student Majors}{18}}
\newlabel{sc:majors}{{4.1}{18}}
\@writefile{toc}{\contentsline {paragraph}{}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Training progress for predicting student majors. Here the hyper-parameters include a learning rate of $\eta = 10^{-2}$, mini-batch size of $1000$, a neural network structure of $3598-500-500-47$, with ReLU activation in the hidden layers, softmax activation in the output layer, negative log-likelihood objective, a dropout rate of $0.5$, and trained for $200$ epochs using Adam algorithm.\relax }}{19}}
\newlabel{fig:major_train}{{4.1}{19}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Prediction of student majors test results with different neural networks.\relax }}{19}}
\newlabel{tab:major_results}{{4.1}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Prediction of Course Selection}{20}}
\newlabel{sc:courses}{{4.2}{20}}
\@writefile{toc}{\contentsline {paragraph}{}{20}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Prediction of student course selection test results with different neural networks.\relax }}{20}}
\newlabel{tab:course_results}{{4.2}{20}}
\citation{Sa09}
\citation{VLLBM10}
\citation{KiWe13}
\bibstyle{plain}
\bibdata{thesis}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Prediction of Student Grades}{21}}
\newlabel{sc:grades}{{4.3}{21}}
\@writefile{toc}{\contentsline {paragraph}{}{21}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces Prediction of student course grade test results with different neural networks.\relax }}{21}}
\newlabel{tab:grade_results}{{4.3}{21}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{21}}
\bibcite{BaRoYo14}{1}
\bibcite{Bi07}{2}
\bibcite{Br01}{3}
\bibcite{DaLa12}{4}
\bibcite{FeHeKh12}{5}
\bibcite{GaGh15}{6}
\bibcite{IoSz15}{7}
\bibcite{KiBa14}{8}
\bibcite{KiWe13}{9}
\bibcite{Le98}{10}
\bibcite{MnSa07}{11}
\bibcite{Ne13}{12}
\bibcite{Sa09}{13}
\bibcite{SaMn08}{14}
\bibcite{SaMnHi07}{15}
\bibcite{SrHi14}{16}
\bibcite{VLLBM10}{17}

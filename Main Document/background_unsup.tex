
\chapter{Unsupervised Learning} \label{ch:unsupervised_learning}

\section{Restricted Boltzmann Machines} \label{sc:rbm}

\paragraph{}
On the other hand, restricted Boltzmann machines (RBM) is a
completely different approach to problems without labels.
RBM is a type of unsupervised learning algorithm, 
for there are no labels to ``supervise'' the learning.
The purpose of unsupervised algorithms are to find
structural patterns within the data itself.
In this case, we are interested in the relationships 
between the performance in difference courses,
and how this helps us predict the grades.

A RBM is a Markov random field in the form of a bipartite graph,
where the joint probability follows a Boltzmann type distribution.
The bipartite graph structure creates two layers without
internal connections.
One layer, called the visible layer, contain the input data; 
in this case, the visible values are the grades of each student.
These nodes are connected to the other layer, called the hidden layer,
with symmetrical weighted connections.
%
\def\layersep{2.5cm}
\begin{figure}[h]
\centering\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=0pt,
  row sep=-1cm
  },
>=latex
]

\matrix[net] (mat)
{
|[plain]| \parbox{1.3cm}{\centering Hidden Nodes $h_j$} & & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| & \\
|[plain]| \parbox{1.3cm}{\centering Weights $w_{ij}$}
    &|[plain]| &|[plain]| &|[plain]| &|[plain]| &
    |[plain]| &|[plain]| &|[plain]| &|[plain]| &|[plain]| \\
|[plain]| \parbox{1.3cm}{\centering Visible Nodes $v_i$} & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| &\\
};
\foreach \ai [count=\mi ]in {3,5,7,9}
  \draw[<-] (mat-3-\ai) -- node[left] {Grade \mi} +(0cm,-1.3);
\foreach \ai in {2,4,...,10}
{\foreach \aii in {3,5,7,9}
  \draw[-] (mat-1-\ai) -- (mat-3-\aii);
}
\end{tikzpicture}

\caption{\label{fig:RBM}
A restricted Boltzmann machine (RBM) with 4 courses and 5 hidden
nodes for a specific student.
}
\end{figure}

Suppose the graph have $N$ visible nodes and $M$ hidden nodes,
with each visible node denoted $v_i$, hidden nodes denoted $h_j$, 
weights between two nodes $w_{ij}$,
$b_i$ and $a_j$ be bias parameters,
and $\sigma_i$ be the standard deviation of grades for each course.
Here each visible node $v_i$ represents the grade for course $i$,
where a specific student is fixed.
Let $\theta = \{ w_{ij},a_j,b_i,\sigma_i \} \; \forall i,j$,
$\mathbf{v} = \{v_i\} \; \forall i$,
and $\mathbf{h} = \{h_j\} \; \forall j$ denote the collections.
Additionally, we let the hidden nodes only take on binary values, 
i.e. $v_i \in \mathbb{R}, h_j \in \{0,1\}$.
We can then define the energy function and 
the joint distribution for the graph:
%
\begin{equation*}
\begin{aligned}
    E(\mathbf{v},\mathbf{h}|\theta) &= 
        \displaystyle\sum_{i=1}^N \frac{(b_i - v_i)^2}{2 \sigma_i}
        - \displaystyle\sum_{i=1}^N \displaystyle\sum_{j=1}^M
             w_{ij} h_j \frac{v_i}{\sigma_i}
        - \displaystyle\sum_{i=j}^M a_j h_j \\
% \end{equation*}
%
% The joint distribution for the graph is then defined
% by the Boltzmann distribution:
%
% \begin{equation*}
    P(\mathbf{v},\mathbf{h}|\theta) &= 
        \frac{\exp\left[-E(\mathbf{v},\mathbf{h}|\theta)\right]}
        {\mathcal{Z}}
\end{aligned}
\end{equation*}
%
where $\mathcal{Z}$ is the partition function normalizing
the distribution.
After marginalizing over the hidden nodes $\mathbf{h}$, 
we can find the gradient of the likelihood function
with respect to the parameters $\theta$
to perform steepest descent optimization.
Finding the gradient requires the use of Gibbs sampling,
although \cite{SaMnHi07} showed the approximate gradient
after very few iterations of Gibbs sampling is sufficient 
for optimization.

\begin{equation*}
    \frac{\de P(\mathbf{v}|\theta)}{\de w_{ij}} = 
        \mathbf{E}_{\text{data}} (v_i h_j) -
        \mathbf{E}_{\text{model}} (v_i h_j)
\end{equation*}

where $\mathbf{E}_{\text{data}}$ refers to expectation 
of observing the case within data,
and $\mathbf{E}_{\text{model}}$ is the expectation of 
the current model with parameters $\theta$.
Instead of using Gibbs sampling until convergence to find 
$\mathbf{E}_{\text{model}}$, 
\cite{SaMnHi07} uses $k$ iterations for a very good approximation
of the gradient.
This method is referred to contrastive divergence (CD) by the 
authors in \cite{SaMnHi07}, where CD-$k$ refers to $k$ iterations
used in Gibbs sampling.
As a result, we have a very good algorithm optimize the
RBM for likelihood.

To perform inference on a missing grade value,
one simply include an additional ``visible'' node $v_p$,
where the value is not known, 
but can be determined by the energy function:
%
\begin{equation*}
\begin{aligned}
    P(v_p|\mathbf{v}) &\propto
        \displaystyle\sum_{\mathbf{h}} 
        \exp[-E(v_p,\mathbf{v},\mathbf{h})] \\
    &= \prod_{j=1}^M \left( 1 + 
        \exp\left[\sum_{i=1}^N w_{ij} v_i\right]
        \right)
\end{aligned}
\end{equation*}












\section{Denoising Autoencoders} \label{sc:dae}

\paragraph{}
Another approach to unsupervised learning is using autoencoders (AEs), 
specifically in this case we will introduce the 
denoising autoencoders (DAEs) in \cite{VLLBM10}.
The autoencoder is a compression model of input data, 
such that a high dimensional input can be encoded as 
a low dimensional representation,
where the data can be reconstructed from the representation 
using a decoder.

For this problem we consider a dataset 
$\mathcal{D} = \{\mathbf{x}^{[n]}\}, 
\mathbf{x}^{[n]} \in \mathbb{R}^{N_{in}},
n \in \mathbb{N}$,
with only $\mathbf{x}$ as the input.
We also define a desired feature $\mathbf{h} \in \mathbb{R}^{N_{feat}} $
with $N_{feat} < N_{in}$,
and an encoder-decoder pair 
$f(\mathbf{x},\mathbf{w}^{(1)}), g(\mathbf{h},\mathbf{w}^{(2)})$
such that the reconstruction 
$\hat{\mathbf{x}} = g \circ f(\mathbf{x}) \approx \mathbf{x}$.
This results in a forced compression of input $\mathbf{x}$ 
into lower dimensional $\mathbf{h}$,
and in the process, the parameterization $\mathbf{w}$ retains 
further information about the data structure.

\def\layersep{2.5cm}
\begin{figure}[H]
\centering\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=0pt,
  row sep=-1cm
  },
>=latex
]

\matrix[net] (mat)
{
|[plain]| \parbox{1.3cm}{\centering Reconstruction\\$\hat{x}_l$} & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| &\\
% |[plain]| \parbox{1.3cm}{\centering Weights \\$w_{kl}^{(3)}$}\\
%     % &|[plain]| &|[plain]| &|[plain]| &|[plain]| &
%     % |[plain]| &|[plain]| &|[plain]| &|[plain]| &|[plain]| \\
% |[plain]| \parbox{1.2cm}{\centering Hidden\\$h_k^{(2)}$} & & 
%     |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| & \\
|[plain]| \parbox{1.3cm}{\centering Weights \\$w_{jl}^{(2)}$}
    &|[plain]| &|[plain]| &|[plain]| &|[plain]| &
    |[plain]| &|[plain]| &|[plain]| &|[plain]| &|[plain]| \\
|[plain]| \parbox{1.2cm}{\centering Representation\\$h_j^{(1)}$} & |[plain]| & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| & |[plain]| \\
|[plain]| \parbox{1.3cm}{\centering Weights \\$w_{ij}^{(1)}$}
    &|[plain]| &|[plain]| &|[plain]| &|[plain]| &
    |[plain]| &|[plain]| &|[plain]| &|[plain]| &|[plain]| \\
|[plain]| \parbox{1.3cm}{\centering Input\\$x_i$} & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| &\\
};
\foreach \ai [count=\mi ]in {3,5,7,9}
  \draw[<-] (mat-5-\ai) -- node[left] {Input $x_\text{\mi}$} +(0cm,-1.3);
\foreach \ai in {3,5,7,9}
{\foreach \aii in {4,6,8}
  \draw[<-] (mat-1-\ai) -- (mat-3-\aii);
}
\foreach \ai in {4,6,8}
{\foreach \aii in {3,5,7,9}
  \draw[<-] (mat-3-\ai) -- (mat-5-\aii);
}
\end{tikzpicture}

\caption{\label{fig:AE1}
An one layer autoencoder with 4 input nodes and 3 representation nodes.
}
\end{figure}

With this setup, we have a neural network described as in Figure \ref{fig:AE1}.
In this structure, we can optimize for the optimal parameters 
$\{\mathbf{w}^{(1)}, \mathbf{w}^{(2)}\}$ using the gradient descent approach
from feedforward neural networks \ref{sc:nnet}. 
In practice, tied weights condition
$\mathbf{w}^{(1)} = [\mathbf{w}^{(2)}]^\top$ 
is often enforced to start the optimization.
Observe that when the weights are tied and 
the non-linear activation function is sigmoid, 
we have a striking similarity with the RBM:
the representation $\mathbf{h}$ is exactly 
the probability of binary hidden layer sampled as 1.

However as \cite{VLLBM10} explained, 
pure compression retains insufficient information,
especially when compared to RBMs;
therefore the authors introduced a new optimization criterion:
reconstruction from noisy inputs.
Formally, we have a corruption function $q$ 
that creates noisy inputs $\mathbf{v} = q(\mathbf{x})$.
A popular choice of $q$ is to randomly 
set a fraction of the input dimensions to zero.

To motivate denoising autoencoders, 
we consider an example with 2 inputs, i.e. $\mathbf{x} = \{x_1, x_2\}$,
and let $x_1 \approx \phi(x_2)$ for some bijection $\phi$.
When $x_1$ is set to zero due to corruption, 
it remains possible to reconstruct $x_1$ by 
learning the relationship between $x_1$ and $x_2$,
which gives us $\hat{x}_1 = \phi(x_2)$.
Similarly, when $x_2$ is corrupted, ideally we can have 
$\hat{x}_2 = \phi^{-1}(x_1)$.


\def\layersep{2.5cm}
\begin{figure}[H]
\centering\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=0pt,
  row sep=-1cm
  },
>=latex
]

\matrix[net] (mat)
{
|[plain]| \parbox{1.3cm}{\centering Reconstruction\\$\hat{x}_l$} & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| &\\
|[plain]| \parbox{1.3cm}{\centering Weights \\$w_{jl}^{(2)}$}
    &|[plain]| &|[plain]| &|[plain]| &|[plain]| &
    |[plain]| &|[plain]| &|[plain]| &|[plain]| &|[plain]| \\
|[plain]| \parbox{1.3cm}{\centering Representation\\$h_j^{(1)}$} & |[plain]| & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| & |[plain]| \\
|[plain]| \parbox{1.3cm}{\centering Weights \\$w_{ij}^{(1)}$}
    &|[plain]| &|[plain]| &|[plain]| &|[plain]| &
    |[plain]| &|[plain]| &|[plain]| &|[plain]| &|[plain]| \\
|[plain]| \parbox{1.3cm}{\centering Noisy Input\\$q(x_i)$} & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| &\\
|[plain]| \parbox{1.3cm}{\centering Corruption \\$q(\cdot)$}
    &|[plain]| &|[plain]| &|[plain]| &|[plain]| &
    |[plain]| & |[plain]| $q(x_3)=0$ &|[plain]| &|[plain]| &|[plain]| \\
|[plain]| \parbox{1.3cm}{\centering Input\\$x_i$} & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| &\\
};
\foreach \ai [count=\mi ]in {3,5,7,9}
  \draw[<-] (mat-7-\ai) -- node[left] {Input $x_\text{\mi}$} +(0cm,-1.3);
\foreach \ai in {3,5,7,9}
  \draw[<-] (mat-5-\ai) -- (mat-7-\ai);
% \draw[<-] (mat-5-7) -- node[left] {$q(x_3) = 0$} +(0cm,-2.8cm);
\foreach \ai in {3,5,7,9}
{\foreach \aii in {4,6,8}
  \draw[<-] (mat-1-\ai) -- (mat-3-\aii);
}
\foreach \ai in {4,6,8}
{\foreach \aii in {3,5,7,9}
  \draw[<-] (mat-3-\ai) -- (mat-5-\aii);
}
\end{tikzpicture}

\caption{\label{fig:DAE1}
An one layer denoising autoencoder with 4 input nodes, 3 representation nodes,
and symmetric weights. In this case, we have the third input corrupted
by setting to zero.
}
\end{figure}

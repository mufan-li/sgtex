% 2. Background

\chapter{Supervised Learning} \label{ch:supervised_learning}

\section{Feed-forward Neural Networks} \label{sc:nnet}

\paragraph{}
We first consider a class of machine learning algorithms
called supervised learning. 
In this case we have a dataset $\mathcal{D} = \{\mathbf{x}^{[n]}, 
\mathbf{y}^{[n]}\}, \mathbf{x}^{[n]} \in \mathbb{R}^{N_{in}}, 
\mathbf{y}^{[n]} \in \mathbb{R}^{N_{out}}, n \in \mathbb{N}$,
with $\mathbf{x}$ as the input, and $\mathbf{y}$ as the label or output.
We want to find a model 
$f(\mathbf{x},\mathbf{w})$ such that 
it is the ``closest'' to $\mathbf{y}$, 
with $\mathbf{w}$ the parameters in the model.
This section will introduce neural networks as the model $f$
that predicts the labels $\mathbf{y}$.

In the simplest case, neural networks can be reduced to 
a generalized linear model (GLM),
where the prediction is a linear combination of inputs
but passed through a non-linear function:
%
\begin{equation*}
	f(\mathbf{x},\mathbf{w}) = g\left( 
    \sum_{i=1}^{N_{in}} w_i x_i + w_0 \right)
\end{equation*}
%
\indent Here $g(\cdot)$ is a non-linear function, 
with $\mathbf{x}$ is an $N$ dimensional input vector,
and $\mathbf{w}$ is the weight vector,
which includes $w_0$ as the bias.
Common choices of $g(\cdot)$, also known as activation functions,
for neural networks 
include the logistic (sigmoid) function, the hyperbolic tangent function, 
and the rectified linear unit (ReLU):
%
\begin{equation*}
\begin{aligned}
	g_{\text{logistic}}(z) &= \frac{1}{1+e^{-z}} \\
	g_{\text{tanh}}(z) &= \tanh(z) = \frac{1 - e^{-2z}}{1 + e^{-2z}}\\
	g_\text{ReLU}(z) &= \max(z,0)
\end{aligned}
\end{equation*}
%
% \indent 
where $z = \left( \sum_{i=1}^N w_i x_i + w_0 \right)$ 
denotes the linear combination for GLMs.
Notice all of these functions have simple derivatives,
and specifically logistic and hyperbolic tangent functions 
are monotonic and bounded by their horizontal asymptotes at infinities,
which makes them great choices for binary classification problems.

Graphically, this can be represented by a series of 
input nodes $\{x_i\}$ connected to an output node $f$,
with weights $\{w_i\}$ on the connections.
Note bias is omitted from the graph but remains a parameter.

\def\layersep{2.5cm}
\begin{figure}[h]
\centering\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=0pt,
  row sep=-1cm
  },
>=latex
]

\matrix[net] (mat)
{
|[plain]| \parbox{1.2cm}{\centering Output Node\\$f$} & |[plain]| & 
    |[plain]| & |[plain]| & |[plain]| & & |[plain]| & |[plain]| & |[plain]| 
    & |[plain]| \\
|[plain]| \parbox{1.3cm}{\centering Weights\\$w_{i}$}
    &|[plain]| &|[plain]| &|[plain]| &|[plain]| &
    |[plain]| &|[plain]| &|[plain]| &|[plain]| &|[plain]| \\
|[plain]| \parbox{1.3cm}{\centering Input Nodes\\$x_i$} & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| &\\
};
\foreach \ai [count=\mi ]in {5,7,9}
  \draw[<-] (mat-3-\ai) -- node[left] {Input $x_\text{\mi}$} +(0cm,-1.3);
\draw[<-] (mat-3-3) -- node[left] {Bias 1} +(0cm,-1.3);
\foreach \ai in {6}
{\foreach \aii in {3,5,7,9}
  \draw[<-] (mat-1-\ai) -- (mat-3-\aii);
}
\end{tikzpicture}

\caption{\label{fig:GLM}
A generalized linear model represented in graphical form.
In a neural network, this is also referred to as a single neuron.
}
\end{figure}

%formatting
% \pagebreak

A general feed-forward neural network is defined by recursive GLMs
with different weights.
For example, a neural network with two hidden layers
(three layers of recursion) is defined as:
%
\begin{equation*}
\begin{aligned}
	h^{(1)}_j &= g^{(1)}
		\left(\sum_{i=1}^{N^{(1)}} w_{ij}^{(1)} x_i + 
      w^{(1)}_{0j} \right) \\
	h^{(2)}_k &= g^{(2)}
		\left(\sum_{j=1}^{N^{(2)}} w_{jk}^{(2)} h_j^{(1)} + 
      w^{(2)}_{0k} \right) \\
	f_l &= g^{(3)}
		\left(\sum_{k=1}^{N^{(3)}} w_{kl}^{(3)} h_k^{(2)} + 
      w^{(3)}_{0l} \right)
\end{aligned}
\end{equation*}
%
where $g(\cdot)^{(\alpha)}$ is some activation function,
$h^{(\alpha)}_j$ denotes the $j^\text{th}$ node of the $\alpha^\text{th}$ 
hidden layer,
$w_{ij}^{(\alpha)}$ denotes the weight for the connection of 
the $i^\text{th}$ node of the $\alpha^\text{th}$ layer to 
the $j^\text{th}$ node of the $(\alpha+1)^\text{th}$ layer,
and $N^{(\alpha)}$ denotes the number of nodes in the $\alpha^\text{th}$ layer.
Additionally, let $N^{(4)}$ be the number of 
output nodes $f_l$,
and $\mathbf{w} = \left[\mathbf{w}^{(1)} \mathbf{w}^{(2)} \mathbf{w}^{(3)} \right]$.
Here we also note that $N^{(1)}$ is the number of input nodes.

Graphically, this structure has a very clear representation:
%
\def\layersep{2.5cm}
\begin{figure}[h]
\centering\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=0pt,
  row sep=-1cm
  },
>=latex
]

\matrix[net] (mat)
{
|[plain]| \parbox{1.3cm}{\centering Output\\$f_l$} & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| &\\
|[plain]| \parbox{1.3cm}{\centering Weights \\$w_{kl}^{(3)}$}\\
    % &|[plain]| &|[plain]| &|[plain]| &|[plain]| &
    % |[plain]| &|[plain]| &|[plain]| &|[plain]| &|[plain]| \\
|[plain]| \parbox{1.2cm}{\centering Hidden\\$h_k^{(2)}$} & & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| & \\
|[plain]| \parbox{1.3cm}{\centering Weights \\$w_{jk}^{(2)}$}
    &|[plain]| &|[plain]| &|[plain]| &|[plain]| &
    |[plain]| &|[plain]| &|[plain]| &|[plain]| &|[plain]| \\
|[plain]| \parbox{1.2cm}{\centering Hidden\\$h_j^{(1)}$} & & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| & \\
|[plain]| \parbox{1.3cm}{\centering Weights \\$w_{ij}^{(1)}$}
    &|[plain]| &|[plain]| &|[plain]| &|[plain]| &
    |[plain]| &|[plain]| &|[plain]| &|[plain]| &|[plain]| \\
|[plain]| \parbox{1.3cm}{\centering Input\\$x_i$} & 
    |[plain]| & & |[plain]| & & |[plain]| & & |[plain]| &\\
};
\foreach \ai [count=\mi ]in {3,5,7,9}
  \draw[<-] (mat-7-\ai) -- node[left] {Input $x_\text{\mi}$} +(0cm,-1.3);
\foreach \ai in {3,5,7,9}
{\foreach \aii in {2,4,...,10}
  \draw[<-] (mat-1-\ai) -- (mat-3-\aii);
}
\foreach \ai in {2,4,...,10}
{\foreach \aii in {2,4,...,10}
  \draw[<-] (mat-3-\ai) -- (mat-5-\aii);
}
\foreach \ai in {2,4,...,10}
{\foreach \aii in {3,5,7,9}
  \draw[<-] (mat-5-\ai) -- (mat-7-\aii);
}
\end{tikzpicture}

\caption{\label{fig:NN}
A generalized feed-forward neural network with two hidden layers. 
Bias parameters are not drawn for compactness, 
although they are present in all forward passing nodes.
}
\end{figure}

% formatting
% \pagebreak 

While most GLMs do not admit a closed-form solution,
a satisfactory optimization can be achieved by the gradient descent method.
In the neural network case,
the optimization becomes more difficult as
the number of parameters increase with the number of nodes and layers.
However, we can still apply the gradient descent method
and find a local optimum for the simpler neural networks. \cite{Bi07}

Once again we have a dataset $\mathcal{D} = \{\mathbf{x}^{[n]}, 
\mathbf{y}^{[n]}\}, n \in \mathbb{N}$,
and we want to find a model $f(\mathbf{x},\mathbf{w})$ such that 
it is the ``closest'' to $\mathbf{y}$.
If the error function $E(f,\mathbf{y})$
and the model $f(\mathbf{x},\mathbf{w})$ are differentiable
with respect to $\mathbf{w}$,
the model can be optimized by gradient descent.
In other words, for any randomly initialized $\mathbf{w}^0$,
an improvement $\mathbf{w}^{k+1}$ can be obtained by making a 
small modification
in the direction of the gradient with respect to $\mathbf{w}^k$ :
%
\begin{equation*}
	\mathbf{w}^{k+1} = \mathbf{w}^{k} - \eta \; \nabla_{\mathbf{w}^k} 
					E\left(f(\mathbf{x},\mathbf{w}^k),\mathbf{y}\right)
\end{equation*}
%
where $\eta > 0$ is hyper-parameter controlling the change of 
each optimization iteration, commonly called the learning rate.
Note $\eta$ is not part of the final model $f(\mathbf{x},\mathbf{w})$,
but it will significantly influence optimization.

In the two hidden layer neural network previously,
a derivative with respect to any weight 
$w_{ij}^{(\alpha)}$ can be found
by applying the chain rule to the derivatives.
For example the derivative with respect to $w_{jk}^{(2)}$ 
where $j \neq 0$ :
%
\begin{equation*}
\begin{aligned}
	\text{let } z_j^{(\alpha)} &= 
		\sum_{i=1}^{N^{(\alpha)}} w_{ij}^{(\alpha)} h_i^{(\alpha-1)}
      + w_{0j}^{(\alpha)} \\
	\text{then } \frac{\de E}{\de w_{jk}^{(2)}} &= 
		\sum_{l=1}^{N^{(4)}} \frac{\de E}{\de f_l}
		\frac{\de f_l}{\de z_l^{(3)}}
		\frac{\de z_l^{(3)}}{\de h_k^{(2)}}
		\frac{\de h_k^{(2)}}{\de z_k^{(2)}}
		\frac{\de z_k^{(2)}}{\de w_{jk}^{(2)}} \\
	&=
		\sum_{l=1}^{N^{(4)}} \frac{\de E}{\de f_l}
		\frac{\de g^{(3)}(z_l^{(3)}) } {\de z_l^{(3)}}
		w_{kl}^{(3)}
		\frac{\de g^{(2)}(z_k^{(2)}) } {\de z_k^{(2)}}
		h_j^{(1)} .
\end{aligned}
\end{equation*}
%
% \indent 
Recall $g^{(\alpha)}(\cdot)$ is selected to have a simple derivative,
making the complex appearing gradient term above easy to compute.

























\section{Common Techniques to Improve Training} \label{sc:tech}

\paragraph{}
While the setup described in the previous section
remains valid and works for simple cases,
several simple techniques can significantly 
improve the speed and quality of optimizing the parameters
in the neural network.
It is important to note these techniques are
also applicable in unsupervised learning techniques 
in Section \ref{ch:unsupervised_learning}.







\subsection{Mini-Batches} \label{subsc:mini_batches}

\paragraph{}
Since the computational complexity of the gradient 
$\nabla_{\mathbf{w}^k}E$ scales linearly with the data size,
and the data tends to be highly similar, 
it is acceptable to approximate the gradient using 
a small portion of the data.
The resulting algorithm is to first divide up 
the input data into smaller batches, 
and then perform a gradient update for every mini-batch 
at random order.

The algorithm is summarized as follows:

\begin{algorithm}[H]
 % \KwData{this text}
 % \KwResult{how to write algorithm with \LaTeX2e }
 Initialize $\mathbf{w}^0, k=0$\;
 {Partition} dataset $\mathcal{D}$ into $N$
    mini-batches $\{\mathcal{D}_n\}_{n=1}^N$ \;
 \Repeat{convergence}{
  {Randomize} the order of mini-batches
    $\{\mathcal{D}_{n'}\}_{n'=1}^N$ \;
  \For{$n' = 1,\ldots,N$}{
    Use
    $\mathbf{x}_{n'}, \mathbf{y}_{n'} \in \mathcal{D}_{n'}$ \;
    $\mathbf{w}^{k+1} = \mathbf{w}^{k} - \eta \; \nabla_{\mathbf{w}^k} 
      E\left(f(\mathbf{x}_{n'},\mathbf{w}^k),\mathbf{y}_{n'}\right)$ \;
    $k = k+1$ \;
  }
 }
 \caption{The Mini-Batch Gradient Descent}
\end{algorithm}

Using mini-batches takes away the need to perform updates 
after iterating through the entire dataset, 
which significantly reduces the computational time.





\subsection{Momentum} \label{subsc:momemtum}

\paragraph{}
Instead of letting the gradient dictate the change in $\mathbf{w}^k$,
the idea of momentum is to let the gradient dictate the rate of change.
If $\mathbf{w}^k$ is interpreted as a coordinate,
and each optimization iteration as velocity,
momentum can be seen as using the gradient as acceleration
instead of velocity. 
This allows the optimization to accumulate speed in 
a consistent direction of the gradient, 
while making it harder to slow down and 
converge to a poor local minimum.

At the same time, 
momentum also improves the speed of convergence.
It is well known that for a convex optimization problem,
the gradient descent method will converge at a rate 
of $\mathcal{O}(k^{-1})$, 
where $k$ is the number of iterations.
The momentum technique is a special case of 
Nesterov Acceleration \cite{Ne13},
where the rate of convergence is 
$\mathcal{O}(k^{-2})$ for a strongly convex 
objective function and a Lipschitz continuous gradient.
Since the neural network objective function is highly
non-convex, the momentum method tends to 
perform better in practice.

The formulation starts with a velocity vector $\mathbf{v}^0$
initialized to zero, and the rest in similar:
%
\begin{equation*}
\begin{aligned}
	\mathbf{v}^{k+1} &= \theta \mathbf{v}^{k} - \eta \; \nabla_{\mathbf{w}^k} 
				E\left(f(\mathbf{x},\mathbf{w}^k),\mathbf{y}\right) \\
	\mathbf{w}^{k+1} &= \mathbf{w}^{k} + \mathbf{v}^{k+1}
\end{aligned}
\end{equation*}
%
where $\theta \in [0,1]$ is the hyper-parameter 
deciding the preservation of momentum.
Here choosing a larger $\theta$ would result in a stronger 
preservation of the velocity vector $\mathbf{v}$,
which then retains more momentum.






\subsection{Dropout} \label{subsc:dropout}

\paragraph{}
Another common issue for neural networks is over-fitting.
Due to the large number of parameters, 
a neural network can tend to ``store'' the entire dataset 
into its parameters, 
hence over-fitting the training dataset.
While regularization and early stopping are used for 
preventing this issue,
we explore a simple yet highly effective 
technique called dropout \cite{SrHi14}.

The motivation for dropout comes from ensemble methods,
where multiple model predictions are averaged,
with each model weighted by its posterior probability 
given the data.
Ensembles are highly successful when
a large number of distinct models can be generated with 
relative low computation.
A notable example is random forests \cite{Br01}
where each model is a simple decision tree.
For neural networks, 
the simplest method to create distinct models 
is by considering different model architectures.
However this is difficult to apply directly
due to the computational cost of optimizing even 
one neural network.

Dropout is the method that attempts to incorporate 
random neural network architecture into 
the same training procedure.
The technique specifically refers to ``dropping out''
some of the hidden units with some probability $p \in [0,1]$,
creating a random structure for each gradient descent 
iteration.

Specifically recall the feed-forward neural network, 
where we originally had
%
\begin{equation*}
  h^{(2)}_k = g^{(2)}
    \left(\sum_{j=1}^{N^{(2)}} w_{jk}^{(2)} h_j^{(1)} + 
      w^{(2)}_{0k} \right)
\end{equation*}

Here for each gradient descent iteration,
we introduce a list of Bernoulli random variables
$r^{(1)}_j \sim $Bernoulli$(1-p)$,
and modify the previous equation to
%
\begin{equation*}
  h^{(2)}_k = g^{(2)}
    \left(\sum_{j=1}^{N^{(2)}} w_{jk}^{(2)} 
    h_j^{(1)} r_j^{(1)}
     + w^{(2)}_{0k} \right)
\end{equation*}

Observe that with probability $p$, 
each hidden node $h_j^{(1)}$ will be set to zero,
hence creating a random structure.
We also observe that since if the node $h_j^{(1)}$
is dropped, we have that the parameter 
$w_{jk}^{(2)}$ stays unchanged for this iteration,
since the hidden node $h_j^{(1)}$ does not 
affect the objective function for this iteration.
Similarly, all the parameters 
$w_{ij}^{(1)},\forall i$ intended for 
the nodes leading up to $h_j^{(1)}$ 
remain unchanged as well.


















\relax 
\citation{BaRoYo14}
\citation{FeHeKh12}
\citation{MnSa07}
\citation{SaMn08}
\citation{Sa09}
\citation{VLLBM10}
\citation{KiWe13}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{sc:intro}{{1}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {paragraph}{}{1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background}{2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:background}{{2}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Supervised Learning Using Feedforward Neural Networks}{2}}
\newlabel{sc:nnet}{{2.1}{2}}
\@writefile{toc}{\contentsline {paragraph}{}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces  A generalized linear model represented in graphical form. In a neural network, this is also referred to as a single neuron. \relax }}{3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:GLM}{{2.1}{3}}
\citation{Bi07}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces  A generalized feed-forward neural network with two hidden layers. Bias parameters are not drawn for compactness, although they are present in all forward passing nodes. \relax }}{4}}
\newlabel{fig:NN}{{2.2}{4}}
\citation{Le98}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}MNIST Hand-Written Digits Example}{6}}
\newlabel{sc:MNIST}{{2.2}{6}}
\@writefile{toc}{\contentsline {paragraph}{}{6}}
\newlabel{fig:mnist_nll}{{2.3a}{7}}
\newlabel{sub@fig:mnist_nll}{{a}{7}}
\newlabel{fig:mnist_err}{{2.3b}{7}}
\newlabel{sub@fig:mnist_err}{{b}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces MNIST hand-written digits modeled using a two hidden layer neural network, with negative log-likelihood and classification error rate computed after each epoch.\relax }}{7}}
\newlabel{fig:mnist_results}{{2.3}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Restricted Boltzmann Machines}{8}}
\newlabel{sc:rbm}{{2.3}{8}}
\@writefile{toc}{\contentsline {paragraph}{}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces  A restricted Boltzmann machine (RBM) with 4 courses and 5 hidden nodes for a specific student. \relax }}{8}}
\newlabel{fig:RBM}{{2.4}{8}}
\citation{SaMnHi07}
\citation{SaMnHi07}
\citation{SaMnHi07}
\citation{VLLBM10}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Denoising Autoencoders}{9}}
\newlabel{sc:dae}{{2.4}{9}}
\@writefile{toc}{\contentsline {paragraph}{}{9}}
\citation{VLLBM10}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces  An one layer autoencoder with 4 input nodes and 3 representation nodes. \relax }}{10}}
\newlabel{fig:AE1}{{2.5}{10}}
\bibstyle{plain}
\bibdata{thesis}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces  An one layer denoising autoencoder with 4 input nodes, 3 representation nodes, and symmetric weights. In this case, we have the third input corrupted by setting to zero. \relax }}{11}}
\newlabel{fig:DAE1}{{2.6}{11}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{11}}
\bibcite{BaRoYo14}{1}
\bibcite{Bi07}{2}
\bibcite{FeHeKh12}{3}
\bibcite{KiWe13}{4}
\bibcite{Le98}{5}
\bibcite{MnSa07}{6}
\bibcite{Sa09}{7}
\bibcite{SaMn08}{8}
\bibcite{SaMnHi07}{9}
\bibcite{VLLBM10}{10}
